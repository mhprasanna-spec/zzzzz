# ğŸš€ TinyLlama Chatbot Deployment on Kubernetes
## ğŸ“Œ Project Overview

This project deploys a **TinyLlama-based AI chatbot** using:

- Kubernetes

- Ollama

- Streamlit

- NodePort Service

The chatbot runs inside a Kubernetes cluster and is exposed externally using NodePort.

---

## ğŸ§  Model Used
**Model:** TinyLlama
**Runtime:** Ollama

The model runs inside the Ollama pod.

---

## ğŸ—ï¸ Architecture

```
Browser
   â†“
NodePort Service
   â†“
Streamlit Pod
   â†“ (ClusterIP Service)
Ollama Pod
   â†“
TinyLlama Model (inside container)

```
---

## ğŸ“¦ Kubernetes Components
## 1ï¸âƒ£ Streamlit Deployment

Frontend UI

Sends user prompts to Ollama service

## 2ï¸âƒ£ Ollama Deployment

Runs TinyLlama model

Handles AI inference requests

## 3ï¸âƒ£ Services

Streamlit â†’ NodePort (External access)

Ollama â†’ ClusterIP (Internal communication only)

---

## âš™ï¸ Deployment Commands

```
kubectl apply -f ollama-deployment.yaml
kubectl apply -f streamlit-deployment.yaml
kubectl apply -f services.yaml
```
## ğŸ” Verify Deployment
```
kubectl get pods
kubectl get svc
```



